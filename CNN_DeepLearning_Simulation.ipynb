{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWyU0mLX1Z/vjIk4xw30s5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiwkumar/UAV_CR/blob/main/CNN_DeepLearning_Simulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjR5gVwC_d06"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "class CNNSpectrumSensor:\n",
        "    def __init__(self, num_secondary_users=4):\n",
        "        self.num_sus = num_secondary_users\n",
        "        self.sensing_time = 0.001  # 1ms sensing interval\n",
        "        self.total_time = 60      # 1 minute simulation\n",
        "        self.num_iterations = int(self.total_time / self.sensing_time)\n",
        "        self.window_size = 128    # Input window size for CNN\n",
        "        self.model = None\n",
        "\n",
        "    def build_cnn_model(self):\n",
        "        \"\"\"Build CNN model for spectrum sensing\"\"\"\n",
        "        model = models.Sequential([\n",
        "            # Input layer\n",
        "            layers.Input(shape=(self.window_size, 6)),  # [I, Q, SNR, path_loss, fading, position]\n",
        "\n",
        "            # Convolutional layers\n",
        "            layers.Conv1D(64, 3, activation='relu', padding='same'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.MaxPooling1D(2),\n",
        "\n",
        "            layers.Conv1D(128, 3, activation='relu', padding='same'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.MaxPooling1D(2),\n",
        "\n",
        "            layers.Conv1D(256, 3, activation='relu', padding='same'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.MaxPooling1D(2),\n",
        "\n",
        "            # Dense layers\n",
        "            layers.Flatten(),\n",
        "            layers.Dense(512, activation='relu'),\n",
        "            layers.Dropout(0.5),\n",
        "            layers.Dense(256, activation='relu'),\n",
        "            layers.Dropout(0.3),\n",
        "            layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "        model.compile(optimizer='adam',\n",
        "                     loss='binary_crossentropy',\n",
        "                     metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def prepare_data(self, df):\n",
        "        \"\"\"Prepare data for CNN\"\"\"\n",
        "        # Create feature matrix\n",
        "        X = np.zeros((len(df), self.window_size, 6))\n",
        "\n",
        "        for i in range(len(df)):\n",
        "            # Generate I/Q samples based on SNR and channel conditions\n",
        "            t = np.linspace(0, self.sensing_time, self.window_size)\n",
        "            snr = df.iloc[i]['snr']\n",
        "            path_loss = df.iloc[i]['path_loss']\n",
        "            fading = df.iloc[i]['fading_magnitude']\n",
        "\n",
        "            # Generate complex signal\n",
        "            signal = np.exp(1j * 2 * np.pi * 1000 * t)  # 1 kHz signal\n",
        "            signal = signal * np.sqrt(10**(snr/10))\n",
        "\n",
        "            # Add channel effects\n",
        "            signal = signal * fading * 10**(-path_loss/20)\n",
        "\n",
        "            # Add noise\n",
        "            noise = np.random.normal(0, 1, self.window_size) + 1j * np.random.normal(0, 1, self.window_size)\n",
        "            noisy_signal = signal + noise\n",
        "\n",
        "            # Fill feature matrix\n",
        "            X[i, :, 0] = noisy_signal.real  # I component\n",
        "            X[i, :, 1] = noisy_signal.imag  # Q component\n",
        "            X[i, :, 2] = snr * np.ones(self.window_size)  # SNR\n",
        "            X[i, :, 3] = path_loss * np.ones(self.window_size)  # Path loss\n",
        "            X[i, :, 4] = fading * np.ones(self.window_size)  # Fading\n",
        "            X[i, :, 5] = np.sqrt(df.iloc[i]['x']**2 + df.iloc[i]['y']**2 + df.iloc[i]['z']**2)  # Distance\n",
        "\n",
        "        # Normalize features\n",
        "        for i in range(X.shape[2]):\n",
        "            scaler = StandardScaler()\n",
        "            X[:, :, i] = scaler.fit_transform(X[:, :, i])\n",
        "\n",
        "        y = df['primary_user_present'].values\n",
        "        return X, y\n",
        "\n",
        "def run_simulation():\n",
        "    try:\n",
        "        # Load dataset\n",
        "        print(\"Loading dataset...\")\n",
        "        df = pd.read_hdf('cr_uav_dataset_10k.h5', key='data')\n",
        "\n",
        "        # Initialize simulation parameters\n",
        "        cnn_sensor = CNNSpectrumSensor(num_secondary_users=4)\n",
        "\n",
        "        # Create SNR bins with wider ranges to ensure sufficient samples\n",
        "        snr_bins = np.arange(-20, 6, 2)  # Changed step size to 2\n",
        "        results = []\n",
        "\n",
        "        # Print dataset statistics\n",
        "        print(f\"Total samples in dataset: {len(df)}\")\n",
        "        print(f\"SNR range in dataset: [{df['snr'].min():.1f}, {df['snr'].max():.1f}]\")\n",
        "\n",
        "        # Run simulation for each SNR bin\n",
        "        for snr_min, snr_max in zip(snr_bins[:-1], snr_bins[1:]):\n",
        "            try:\n",
        "                # Filter dataset for current SNR range\n",
        "                df_snr = df[(df['snr'] >= snr_min) & (df['snr'] < snr_max)]\n",
        "\n",
        "                print(f\"Processing SNR range [{snr_min}, {snr_max}] dB with {len(df_snr)} samples\")\n",
        "\n",
        "                if len(df_snr) < 100:  # Reduced minimum sample requirement\n",
        "                    print(f\"Insufficient samples for SNR range [{snr_min}, {snr_max}] dB\")\n",
        "                    continue\n",
        "\n",
        "                # Prepare data\n",
        "                X, y = cnn_sensor.prepare_data(df_snr)\n",
        "\n",
        "                # Split data\n",
        "                split_idx = int(0.8 * len(X))\n",
        "                X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "                y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "                # Build and train model\n",
        "                cnn_sensor.model = cnn_sensor.build_cnn_model()\n",
        "\n",
        "                # Train model\n",
        "                history = cnn_sensor.model.fit(\n",
        "                    X_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2,\n",
        "                    verbose=0\n",
        "                )\n",
        "\n",
        "                # Evaluate performance\n",
        "                y_pred = cnn_sensor.model.predict(X_test, verbose=0)\n",
        "                fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
        "\n",
        "                # Calculate cooperative detection (OR-rule for multiple SUs)\n",
        "                coop_detection = 1 - np.prod([1-tpr[1] for _ in range(cnn_sensor.num_sus)])\n",
        "\n",
        "                # Store results using average SNR of the bin\n",
        "                avg_snr = (snr_min + snr_max) / 2\n",
        "                results.append({\n",
        "                    'snr': avg_snr,\n",
        "                    'snr_min': snr_min,\n",
        "                    'snr_max': snr_max,\n",
        "                    'num_samples': len(df_snr),\n",
        "                    'detection_prob': coop_detection,\n",
        "                    'false_alarm_prob': fpr[1],\n",
        "                    'training_accuracy': history.history['accuracy'][-1],\n",
        "                    'validation_accuracy': history.history['val_accuracy'][-1]\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing SNR range [{snr_min}, {snr_max}] dB: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        if not results:\n",
        "            raise ValueError(\"No valid results generated from any SNR range\")\n",
        "\n",
        "        # Convert results to DataFrame\n",
        "        results_df = pd.DataFrame(results)\n",
        "\n",
        "        # Sort results by SNR\n",
        "        results_df = results_df.sort_values('snr')\n",
        "\n",
        "        # Create plots\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.errorbar(results_df['snr'], results_df['detection_prob'],\n",
        "                    xerr=(results_df['snr_max'] - results_df['snr_min'])/2,\n",
        "                    fmt='o-', label='Cooperative Detection',\n",
        "                    linewidth=2, markersize=8, capsize=5)\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "        plt.xlabel('SNR (dB)', fontsize=12)\n",
        "        plt.ylabel('Probability of Detection', fontsize=12)\n",
        "        plt.title(f'CNN-based Detection Performance vs SNR\\n({cnn_sensor.num_sus} Secondary Users)',\n",
        "                  fontsize=14, pad=20)\n",
        "        plt.legend(fontsize=10)\n",
        "        plt.xlim(min(results_df['snr'])-2, max(results_df['snr'])+2)\n",
        "        plt.ylim(0, 1.1)\n",
        "        plt.grid(True)\n",
        "        plt.savefig('cnn_simulation_results.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        # Plot training metrics\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        # Training/Validation Accuracy\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.plot(results_df['snr'], results_df['training_accuracy'], 'o-',\n",
        "                label='Training', linewidth=2, markersize=6)\n",
        "        plt.plot(results_df['snr'], results_df['validation_accuracy'], 's-',\n",
        "                label='Validation', linewidth=2, markersize=6)\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "        plt.xlabel('SNR (dB)', fontsize=12)\n",
        "        plt.ylabel('Accuracy', fontsize=12)\n",
        "        plt.title('Model Accuracy vs SNR', fontsize=14)\n",
        "        plt.legend(fontsize=10)\n",
        "        plt.ylim(0, 1.1)\n",
        "\n",
        "        # Sample Distribution\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.bar(results_df['snr'], results_df['num_samples'])\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "        plt.xlabel('SNR (dB)', fontsize=12)\n",
        "        plt.ylabel('Number of Samples', fontsize=12)\n",
        "        plt.title('Sample Distribution', fontsize=14)\n",
        "\n",
        "        # ROC-like plot\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.plot(results_df['false_alarm_prob'], results_df['detection_prob'], 'o-',\n",
        "                linewidth=2, markersize=6, color='green')\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "        plt.xlabel('Probability of False Alarm', fontsize=12)\n",
        "        plt.ylabel('Probability of Detection', fontsize=12)\n",
        "        plt.title('Detection vs False Alarm', fontsize=14)\n",
        "        plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
        "        plt.axis('square')\n",
        "        plt.xlim(0, 1)\n",
        "        plt.ylim(0, 1)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('cnn_training_metrics.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        # Print performance summary\n",
        "        print(\"\\nCNN Simulation Results Summary\")\n",
        "        print(\"-----------------------------\")\n",
        "        print(f\"Number of Secondary Users: {cnn_sensor.num_sus}\")\n",
        "        print(f\"Total Simulation Time: {cnn_sensor.total_time} seconds\")\n",
        "        print(f\"Sensing Interval: {cnn_sensor.sensing_time} seconds\")\n",
        "        print(\"\\nPerformance Metrics:\")\n",
        "        print(results_df.round(4))\n",
        "\n",
        "        # Save results\n",
        "        results_df.to_csv('cnn_simulation_results.csv', index=False)\n",
        "\n",
        "        return results_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Simulation failed: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seeds for reproducibility\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    # Run simulation\n",
        "    results_df = run_simulation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZfAVft8FaS1",
        "outputId": "9473e1fa-60f7-4ec6-906d-423defeb282e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n",
            "Total samples in dataset: 10000\n",
            "SNR range in dataset: [-20.0, 5.0]\n",
            "Processing SNR range [-20, -18] dB with 801 samples\n",
            "Processing SNR range [-18, -16] dB with 808 samples\n",
            "Processing SNR range [-16, -14] dB with 745 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7c09b32bdb40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing SNR range [-14, -12] dB with 844 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7c09b29f9d80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing SNR range [-12, -10] dB with 800 samples\n",
            "Processing SNR range [-10, -8] dB with 835 samples\n",
            "Processing SNR range [-8, -6] dB with 779 samples\n",
            "Processing SNR range [-6, -4] dB with 787 samples\n",
            "Processing SNR range [-4, -2] dB with 750 samples\n",
            "Processing SNR range [-2, 0] dB with 813 samples\n",
            "Processing SNR range [0, 2] dB with 817 samples\n",
            "Processing SNR range [2, 4] dB with 832 samples\n",
            "\n",
            "CNN Simulation Results Summary\n",
            "-----------------------------\n",
            "Number of Secondary Users: 4\n",
            "Total Simulation Time: 60 seconds\n",
            "Sensing Interval: 0.001 seconds\n",
            "\n",
            "Performance Metrics:\n",
            "     snr  snr_min  snr_max  num_samples  detection_prob  false_alarm_prob  \\\n",
            "0  -19.0      -20      -18          801          0.0000            0.0133   \n",
            "1  -17.0      -18      -16          808          0.0000            0.0122   \n",
            "2  -15.0      -16      -14          745          0.0503            0.0000   \n",
            "3  -13.0      -14      -12          844          0.0523            0.0000   \n",
            "4  -11.0      -12      -10          800          0.0468            0.0000   \n",
            "5   -9.0      -10       -8          835          0.0000            0.0115   \n",
            "6   -7.0       -8       -6          779          0.0523            0.0000   \n",
            "7   -5.0       -6       -4          787          0.0452            0.0000   \n",
            "8   -3.0       -4       -2          750          0.0000            0.0139   \n",
            "9   -1.0       -2        0          813          0.0485            0.0000   \n",
            "10   1.0        0        2          817          0.0452            0.0000   \n",
            "11   3.0        2        4          832          0.0479            0.0000   \n",
            "\n",
            "    training_accuracy  validation_accuracy  \n",
            "0              0.8906               0.4688  \n",
            "1              0.9322               0.5692  \n",
            "2              0.9538               0.5667  \n",
            "3              0.9222               0.4593  \n",
            "4              0.9023               0.4375  \n",
            "5              0.9363               0.5299  \n",
            "6              0.9096               0.5680  \n",
            "7              0.9006               0.4683  \n",
            "8              0.9250               0.4750  \n",
            "9              0.8731               0.4923  \n",
            "10             0.9291               0.5115  \n",
            "11             0.8966               0.5188  \n"
          ]
        }
      ]
    }
  ]
}